#encoding=utf-8

import requests
from bs4 import BeautifulSoup
import time

'''
目标POC
功能：
	1.爬取最新漏洞
	2.爬取所有漏洞
	3.搜索漏洞

分析：
	1.捕捉搜索漏洞的HTTP，用来搜索漏洞
	2.站点存在防爬机制，用headers绕过
	3.站点存在防DOS攻击，需要添加time实现一段时间的爬取

待实现功能：
	4.显示漏洞是否有POC
	5.获取指定漏洞的详细描述和POC
	6.添加参数的定义和分析，通过参数执行不同的功能
'''
headers = {"User-Agent":"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"}

#爬取漏洞总页数
def load_pagecount():
	url = "https://www.seebug.org/vuldb/vulnerabilities"
	r = requests.get(url = url, headers = headers)
	soup = BeautifulSoup(r.text, "html.parser")
	inputs = soup.find(name = 'input', attrs = {'type':'number'})
	return inputs['max']

#爬取所有漏洞
def load_all_exploit():
	url = "https://www.seebug.org/vuldb/vulnerabilities?page="
	page_count = load_pagecount()
	for i in range(int(page_count)):
		urls = url + str(i)
		r = requests.get(url = urls, headers = headers)
		bs_exploit(r.text)
		time.sleep(1)

#爬取最新漏洞（第一页的漏洞）
def load_new_exploit():
	url = "https://www.seebug.org/vuldb/vulnerabilities"
	r = requests.get(url = url, headers = headers)
	bs_exploit(r.text)

#搜索特定漏洞
def search_bug(bug):
	i = 1
	url = "https://www.seebug.org/search/?keywords=" + bug + '&page='
	while True:
		urls = url + str(i)
		r = requests.get(url = urls, headers = headers)
		if r.status_code == 404:
			break
		else:
			i = i + 1
			bs_exploit(r.text)
			time.sleep(1)

#显示漏洞信息
def bs_exploit(html):
	soup = BeautifulSoup(html, "html.parser")
	exploits = soup.find_all(name = 'a', attrs = {'class':'vul-title',})
	for exploit in exploits:
		print('https://www.seebug.org' + exploit['href'], '\t', exploit['title'])

#展示漏洞poc

def main():
	search_bug('mysql')#参数为待搜索的漏洞
	load_new_exploit()
	load_all_exploit()

if __name__ == '__main__':
	main()
